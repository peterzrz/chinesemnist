{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "        \n",
    "def convert(code):\n",
    "    if (code >= 1 and code <= 11): return (code - 1)\n",
    "    elif (code == 12): return 100\n",
    "    elif (code == 13): return 1000\n",
    "    elif (code == 14): return 10000\n",
    "    elif (code == 15): return 100000000\n",
    "    else: return -1\n",
    "\n",
    "class image:\n",
    "    def __init__(self,suite, sample, code):\n",
    "        self.suite = suite\n",
    "        self.sample = sample\n",
    "        self.code = code\n",
    "        self.value = convert(code)\n",
    "        img_name = \"chinese_data/input_\" + str(suite) + \"_\" + str(sample) + \"_\" + str(code) + \".jpg\"\n",
    "        infile = Image.open(img_name)\n",
    "        temp = np.array(infile)\n",
    "        self.pixels = [col for row in temp for col in row] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "#inverse cdf function\n",
    "def inverse_normal_cdf(num):\n",
    "    return norm.ppf(norm.cdf(num))\n",
    "\n",
    "#necessary tensor functions\n",
    "def is_1d(tensor):\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "def tensor_sum(tensor):\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
    "                   for tensor_i in tensor)   # and sum up those results.\n",
    "    \n",
    "def tensor_combine(f, t1, t2):\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x, y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i)\n",
    "                for t1_i, t2_i in zip(t1, t2)]\n",
    "    \n",
    "def true_encode(tag):\n",
    "    return [1.0 if tag - 1 == i else 0.0 for i in range(15)]\n",
    "\n",
    "def dot(l1, l2):\n",
    "    sum = 0.0\n",
    "    for x,y in zip(l1,l2):\n",
    "        sum += x * y\n",
    "    return sum\n",
    "\n",
    "def tensor_apply(f, tensor):\n",
    "    if is_1d(tensor):\n",
    "        return [f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, t) for t in tensor]\n",
    "    \n",
    "def zeros_like(tensor):\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxIndex(tensor):\n",
    "    max = tensor[0]\n",
    "    maxIndex = 0\n",
    "    for i in range(len(tensor)):\n",
    "        if tensor[i] > max:\n",
    "            max = tensor[i]\n",
    "            maxIndex = i\n",
    "    return maxIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# optimizer class\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, learning_rate: float = 0.1):\n",
    "        self.rate = learning_rate\n",
    "        \n",
    "    # step for each layer\n",
    "    def step(self, layer):\n",
    "        for param, grad in zip(layer.params(), layer.grads()):\n",
    "            # Update param using a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda param, grad: param - grad * self.rate, param, grad)\n",
    "            \n",
    "\n",
    "class Momentum():\n",
    "    def __init__(self,\n",
    "                 learning_rate,\n",
    "                 momentum: float = 0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.mo = momentum\n",
    "        self.updates = []  # running average\n",
    "\n",
    "    def step(self, layer):\n",
    "        # If we have no previous updates, start with all zeros\n",
    "        if not self.updates:\n",
    "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
    "\n",
    "        for update, param, grad in zip(self.updates,\n",
    "                                       layer.params(),\n",
    "                                       layer.grads()):\n",
    "            # Apply momentum\n",
    "            update[:] = tensor_combine(\n",
    "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
    "                update,\n",
    "                grad)\n",
    "\n",
    "            # Then take a gradient step\n",
    "            param[:] = tensor_combine(\n",
    "                lambda p, u: p - self.lr * u,\n",
    "                param,\n",
    "                update)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss class\n",
    "\n",
    "class SSE:\n",
    "    def loss(self, predict, real):\n",
    "        squared_errors = tensor_combine(\n",
    "            lambda predicted, actual: (predicted - actual) ** 2,\n",
    "            predict, real)\n",
    "        return tensor_sum(squared_errors)\n",
    "    def gradient(self, predict, real):\n",
    "        return tensor_combine(lambda predicted, actual:  2 * (predicted - actual),\n",
    "            predict, real)\n",
    "    \n",
    "\n",
    "    \n",
    "def softmax(tensor):\n",
    "    if is_1d(tensor):\n",
    "        # Subtract largest value for numerical stability.\n",
    "        largest = max(tensor)\n",
    "        exps = [math.exp(x - largest) for x in tensor]\n",
    "\n",
    "        sum_of_exps = sum(exps)                 # This is the total \"weight.\"\n",
    "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
    "                for exp_i in exps]              # of the total weight.\n",
    "    else:\n",
    "        return [softmax(tensor_i) for tensor_i in tensor]    \n",
    "class SoftmaxCrossEntropy():\n",
    "\n",
    "    def loss(self, predicted, actual):\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
    "                                     probabilities,\n",
    "                                     actual)\n",
    "\n",
    "        # And then we just sum up the negatives.\n",
    "        return -tensor_sum(likelihoods)\n",
    "\n",
    "    def gradient(self, predicted, actual):\n",
    "        probabilities = softmax(predicted)\n",
    "\n",
    "        return tensor_combine(lambda p, actual: p - actual,probabilities,actual)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# MNIST class\n",
    "class mnist:\n",
    "    def __init__(self, partition):\n",
    "        self.img_array = []\n",
    "        #read in all images\n",
    "        for i in range(1, 31):\n",
    "            for j in range (1, 11):\n",
    "                for k in range (1, 16):\n",
    "                    self.img_array.append(image(i,j,k))\n",
    "        random.shuffle(self.img_array)\n",
    "        cut = int(len(self.img_array) * partition)\n",
    "        # partition the data into train and test\n",
    "        self.train_tag = [self.img_array[i].code for i in range(cut)]\n",
    "        train_img = [self.img_array[i].pixels for i in range(cut)]\n",
    "        self.test_tag = [self.img_array[i].code for i in range(cut, len(self.img_array))]\n",
    "        test_img = [self.img_array[i].pixels for i in range(cut, len(self.img_array))]\n",
    "        # flatten the pixels in train and test\n",
    "        train_avg = tensor_sum(train_img)/ len(train_img) / 64/ 64\n",
    "        test_avg = tensor_sum(test_img)/ len(test_img) / 64/ 64\n",
    "        self.train_img = [[(pixel - train_avg)/256 for pixel in image] for image in train_img]\n",
    "        self.test_img = [[(pixel - test_avg)/256 for pixel in image] for image in test_img]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout class\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.train = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            # Create a mask of 0s and 1s shaped like the input\n",
    "            # using the specified probability.\n",
    "            self.mask = tensor_apply(\n",
    "                lambda _: 0 if random.random() < self.p else 1,\n",
    "                input)\n",
    "            # Multiply by the mask to dropout inputs.\n",
    "            return tensor_combine(operator.mul, input, self.mask)\n",
    "        else:\n",
    "            # During evaluation just scale down the outputs uniformly.\n",
    "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        if self.train:\n",
    "            # Only propagate the gradients where mask == 1.\n",
    "            return tensor_combine(operator.mul, gradient, self.mask)\n",
    "        else:\n",
    "            raise RuntimeError(\"don't call backward when not in train mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import operator\n",
    "# Tensor class\n",
    "\n",
    "def random_uniform(dims):\n",
    "    if len(dims) == 1:\n",
    "        return [random.random() for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_uniform(dims[1:]) for _ in range(dims[0])]\n",
    "\n",
    "def random_normal(dims, mean: float = 0.0, variance: float = 1.0):\n",
    "    if len(dims) == 1:\n",
    "        return [mean + variance * inverse_normal_cdf(random.random())\n",
    "                for _ in range(dims[0])]\n",
    "    else:\n",
    "        return [random_normal(dims[1:], mean, variance)for _ in range(dims[0])]\n",
    "    \n",
    "def random_tensor(dims, init: str =\"normal\"):\n",
    "    if init == 'normal':\n",
    "        return random_normal(dims)\n",
    "    elif init == 'uniform':\n",
    "        return random_uniform(dims)\n",
    "    elif init == 'xavier':\n",
    "        variance = len(dims) / sum(dims)\n",
    "        return random_normal(dims, variance=variance)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown init: {init}\")\n",
    "        \n",
    "# tensor layers setup\n",
    "\n",
    "# base class\n",
    "class Layer():\n",
    "    def forward(self, input):\n",
    "        return []\n",
    "    def backward(self, gradient):\n",
    "        return []\n",
    "    def params(self): return []\n",
    "    def grads(self): return []\n",
    "    \n",
    "\n",
    "# Sigmoid class\n",
    "def sigmoid(num):\n",
    "    return 1 / (1 + math.exp(num * -1))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    # help bring the result into the range between 0 and 1\n",
    "    def forward(self, input):\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,gradient)\n",
    "\n",
    "# Tanh class\n",
    "def tanh(x):\n",
    "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
    "    # We check for this because, e.g., math.exp(1000) raises an error.\n",
    "    if x < -100:  return -1\n",
    "    elif x > 100: return 1\n",
    "\n",
    "    em2x = math.exp(-2 * x)\n",
    "    return (1 - em2x) / (1 + em2x)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, input):\n",
    "        # Save tanh output to use in backward pass.\n",
    "        self.tanh = tensor_apply(tanh, input)\n",
    "        return self.tanh\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        return tensor_combine( lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
    "            self.tanh,\n",
    "            gradient)\n",
    "    \n",
    "    \n",
    "#Linear Class\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim, output_dim, init: str = 'xavier'):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # weights for the each neuron\n",
    "        # output dimension is the number of neurons, and input dimension is the dimension of each neuron\n",
    "        self.weight = random_tensor([output_dim, input_dim], init)\n",
    "        # bias term for each neuron\n",
    "        self.bias = random_tensor([output_dim, 1], init)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        # go through the layer\n",
    "        return [dot(input, self.weight[neuron]) + self.bias[neuron] for neuron in range(self.output_dim)]\n",
    "        \n",
    "    def backward(self, gradient):\n",
    "        self.bias_grad = [[element] for element in gradient]\n",
    "        # the gradient passed into the argument has a dimension of output_dim\n",
    "        # we want a weight gradient with a dimension of output_dim by input_dim\n",
    "        self.weight_grad = [[self.input[i] * gradient[neuron] for i in range(self.input_dim)]\n",
    "                       for neuron in range(self.output_dim)]\n",
    "\n",
    "        # In the forward part, every input i was multiplied with weight[o][i] for each neuron o\n",
    "        # So we reverse that step here to get the backward result\n",
    "        return [sum(self.weight[o][i] * gradient[o] for o in range(self.output_dim))\n",
    "                for i in range(self.input_dim)]\n",
    "        \n",
    "    def params(self):\n",
    "        return [self.weight, self.bias]\n",
    "    def grads(self):\n",
    "        return [self.weight_grad, self.bias_grad]\n",
    "\n",
    "    \n",
    "# The entire model\n",
    "class Model(Layer):\n",
    "    # the key model of our neural network to connect each layer together\n",
    "    # similarly, a derived class of layer in order to incorporate all kinds of potential layers\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    def forward(self, input):\n",
    "        # just go through all the layers\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for i in range(len(self.layers)):\n",
    "            gradient = self.layers[len(self.layers) - i - 1].backward(gradient)\n",
    "        return gradient\n",
    "    def params(self): \n",
    "        return [param for layer in self.layers for param in layer.params()]\n",
    "    def grads(self): \n",
    "        return [grad for layer in self.layers for grad in layer.grads()]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "net = Model([Linear(4096, 64), Dropout(0.1), Tanh(), Linear(64, 15)])\n",
    "data = mnist(0.8)\n",
    "train_true_encode = [true_encode(tag) for tag in data.train_tag]\n",
    "test_true_encode = [true_encode(tag) for tag in data.test_tag]\n",
    "print(\"Hello\")\n",
    "\n",
    "loss = SoftmaxCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " acc: 0.47888888888888886, mnist loss: 1.7414599108366808: 100%|█| 3600/3600 [1\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "optimizer = Momentum(0.1, 0.9)\n",
    "total_loss = 0\n",
    "num_correct = 0.0\n",
    "with tqdm.trange(len(data.train_img)) as t:\n",
    "    for i in t:\n",
    "        prediction = net.forward(data.train_img[i])\n",
    "        if (findMaxIndex(prediction) == findMaxIndex(train_true_encode[i])):\n",
    "            num_correct += 1\n",
    "        total_loss += loss.loss(prediction, train_true_encode[i])\n",
    "        gradient = loss.gradient(prediction, train_true_encode[i])\n",
    "        net.backward(gradient)\n",
    "        optimizer.step(net)\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        acc = num_correct / (i + 1)\n",
    "        t.set_description(f\" acc: {acc}, mnist loss: {avg_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "print (len(net.params()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " acc: 0.4022222222222222, mnist loss: 2.2284028884611105: 100%|█| 900/900 [02:2\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "total_loss = 0.0\n",
    "num_correct = 0.0\n",
    "with tqdm.trange(len(data.test_img)) as t:\n",
    "    for i in t:\n",
    "        prediction = net.forward(data.test_img[i])\n",
    "        if (findMaxIndex(prediction) == findMaxIndex(test_true_encode[i])):\n",
    "            num_correct += 1\n",
    "        total_loss += loss.loss(prediction, test_true_encode[i])\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "        acc = num_correct / (i + 1)\n",
    "        t.set_description(f\" acc: {acc}, mnist loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction = net.forward(data.test_img[0])\n",
    "print (prediction)\n",
    "print(test_true_encode[0])\n",
    "print(loss.loss(prediction, test_true_encode[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"temp_chinese.txt\", \"w\")\n",
    "for i in range(15):\n",
    "    for j in range(4096):\n",
    "        f.write(f\"{net.params()[0][i][j]} \")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.write(\"\\n\")\n",
    "\n",
    "for i in range(15):\n",
    "        f.write(f\"{net.params()[1][i][0]} \")\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
